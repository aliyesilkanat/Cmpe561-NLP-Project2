{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,subprocess\n",
    "def generate_model_line(s,idx): #generates svm input vectors using predefined character numbers\n",
    "    sentence=[x for x in list(''.join(set(''.join(s.split('\\t')[0])))) if x not in insignificant_tokens]\n",
    "    model_line =str(idx+1)\n",
    "    character_list=[]\n",
    "    for ch in sentence:\n",
    "        character_list.append(character_numbers[ch])\n",
    "    character_list.sort()\n",
    "    for ch in character_list:\n",
    "        model_line+=\" \"+ str(ch)+ \":\" + str(1)\n",
    "    return model_line+\"\\n\"\n",
    "\n",
    "insignificant_tokens=[' ','!', '\"','#','$','%','&','*','+','-','(',')',',','.','/','0','1','2','3','4','5','6','7','8',\n",
    "                      '9',';','<','>','=','?','@','|','«','»','`','[',']',\"'\",'\\\\']#these tokens will be skipped \n",
    "language_ids=['bg','bs','cz','es-AR','es-ES','hr','id','mk','my','pt-BR','pt-PT','sk','sr']\n",
    "with open (\"Corpus/Raw Corpus.txt\") as f:\n",
    "    corpus = f.readlines()\n",
    "languages=[]\n",
    "corpus_s=list(''.join(set(''.join(corpus))))\n",
    "corpus_s=[x for x in corpus_s if x not in insignificant_tokens]\n",
    "character_numbers={}\n",
    "for s in corpus_s:\n",
    "    character_numbers[s]=ord(s) #assign each character to a number using ordinal of a one-character string.\n",
    "\n",
    "for i in range(13): #divides data set as languages\n",
    "    languages.append(\"\")\n",
    "    start=i*2000\n",
    "    end=(i+1)*2000\n",
    "    languages[i]=corpus[start:end] #2k sentence for each language\n",
    "\n",
    "training_set=[]\n",
    "test_set=[]\n",
    "training_model=[]\n",
    "test_model=[]\n",
    "for idx,l in enumerate(languages): #assign training and test partitions to each language\n",
    "    random.shuffle(l)\n",
    "    \n",
    "    training_partition=l[0:1800]\n",
    "    training_set.extend(training_partition)\n",
    "    \n",
    "    for s in training_partition:\n",
    "        training_model.append(generate_model_line(s,idx))\n",
    "    \n",
    "    test_partition=l[1800:2000]\n",
    "    test_set.extend(test_partition)\n",
    "    \n",
    "    for s in test_partition:\n",
    "        test_model.append(generate_model_line(s,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23400\n",
      "2600\n"
     ]
    }
   ],
   "source": [
    "def write_files(file_name,array):\n",
    "    with open (file_name, mode='wt') as t_file:\n",
    "        for item in array:\n",
    "            t_file.write(item)\n",
    "print(len(training_set))\n",
    "print(len(test_set))\n",
    "write_files(\"SVM/TrainingModel-SVM.txt\",training_model) #svm inputs as training set\n",
    "write_files(\"SVM/TrainingSet-SVM.txt\",training_set) #which sentences in the training set\n",
    "write_files(\"SVM/TestSet-SVM.txt\",test_set) #svm inputs as test set\n",
    "write_files(\"SVM/TestModel-SVM.txt\",test_model) #which sentences in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=subprocess.Popen(['SVM/svm_multiclass_learn', '-c', '5000' ,'SVM/TrainingModel-SVM.txt' ,'SVM/Model'],\n",
    "                 stdout=subprocess.PIPE) #train svm using shell command\n",
    "p.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model...done.\n",
      "\n",
      "Reading test examples... (2600 examples) done.\n",
      "\n",
      "Classifying test examples...done\n",
      "\n",
      "Runtime (without IO) in cpu-seconds: 0.01\n",
      "\n",
      "Average loss on test set: 31.6538\n",
      "\n",
      "Zero/one-error on test set: 31.65% (1777 correct, 823 incorrect, 2600 total)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=subprocess.Popen(['SVM/svm_multiclass_classify', 'SVM/TestModel-SVM.txt' ,'SVM/Model','SVM/predictions.txt'],\n",
    "                 stdout=subprocess.PIPE) #classify using test set using shell command\n",
    "for line in p.stdout:\n",
    "    print(line)\n",
    "p.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\n",
      "True positives: 196\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 4\n",
      "hr\n",
      "True positives: 124\n",
      "False positives: 74\n",
      "True negatives: 2326\n",
      "False negatives: 76\n",
      "es-AR\n",
      "True positives: 71\n",
      "False positives: 27\n",
      "True negatives: 2373\n",
      "False negatives: 129\n",
      "mk\n",
      "True positives: 200\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 0\n",
      "sk\n",
      "True positives: 195\n",
      "False positives: 5\n",
      "True negatives: 2395\n",
      "False negatives: 5\n",
      "cz\n",
      "True positives: 195\n",
      "False positives: 1\n",
      "True negatives: 2399\n",
      "False negatives: 5\n",
      "sr\n",
      "True positives: 98\n",
      "False positives: 110\n",
      "True negatives: 2290\n",
      "False negatives: 102\n",
      "pt-BR\n",
      "True positives: 95\n",
      "False positives: 63\n",
      "True negatives: 2337\n",
      "False negatives: 105\n",
      "bs\n",
      "True positives: 44\n",
      "False positives: 42\n",
      "True negatives: 2358\n",
      "False negatives: 156\n",
      "my\n",
      "True positives: 143\n",
      "False positives: 5\n",
      "True negatives: 2395\n",
      "False negatives: 57\n",
      "pt-PT\n",
      "True positives: 124\n",
      "False positives: 92\n",
      "True negatives: 2308\n",
      "False negatives: 76\n",
      "es-ES\n",
      "True positives: 183\n",
      "False positives: 129\n",
      "True negatives: 2271\n",
      "False negatives: 17\n",
      "id\n",
      "True positives: 109\n",
      "False positives: 62\n",
      "True negatives: 2338\n",
      "False negatives: 91\n"
     ]
    }
   ],
   "source": [
    "with open (\"SVM/predictions.txt\") as f:\n",
    "    predictions = f.readlines()\n",
    "predictions=[i.split(' ')[0] for i in predictions]\n",
    "metrics={}\n",
    "for idx,lang_id in enumerate(language_ids):\n",
    "    false_negatives=len([x for x in predictions[idx*200:(idx+1)*200] if int(x) != (idx+1)])\n",
    "    true_positives=200-false_negatives\n",
    "    if idx==0: #count false positives on proceeding predictions\n",
    "        false_positives=len([x for x in predictions[200:len(predictions)] if int(x) ==1])\n",
    "    elif idx==12: #count false positives on preceeding predictions\n",
    "        false_positives=len([x for x in predictions[0:2400] if int(x)==13])\n",
    "    else: #count false positives on both preceeding and proceeding predictions\n",
    "        false_positives=[x for x in predictions[(idx-1)*200:idx*200] if int(x) == (idx+1)]\n",
    "        false_positives=len(false_positives + [x for x in predictions[(idx+1)*200:2600] if int(x) == (idx+1)])\n",
    "    true_negatives=2400-false_positives\n",
    "    metrics[lang_id]={\"tp\":true_positives,\"fn\":false_negatives,\"tn\":true_negatives,\"fp\":false_positives}\n",
    "for key,value in metrics.iteritems():\n",
    "    print(key)\n",
    "    print(\"True positives: \" + str(value[\"tp\"]))\n",
    "    print(\"False positives: \" + str(value[\"fp\"]))\n",
    "    print(\"True negatives: \" + str(value[\"tn\"]))\n",
    "    print(\"False negatives: \" + str(value[\"fn\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged precision: 0.744449099288\n",
      "Micro-averaged recall: 0.683461538462\n",
      "Micro-averaged f1-score: 0.712652897534\n",
      "\n",
      "Macro-averaged precision: 0.743765662284\n",
      "Macro-averaged recall: 0.683461538462\n",
      "Macro-averaged f1-score: 0.699124738159\n",
      "\n",
      "Total accuracy: 0.683461538462\n",
      "Accuracies for languages:\n",
      "bg: 0.98\n",
      "hr: 0.62\n",
      "es-AR: 0.355\n",
      "mk: 1.0\n",
      "sk: 0.975\n",
      "cz: 0.975\n",
      "sr: 0.49\n",
      "pt-BR: 0.475\n",
      "bs: 0.22\n",
      "my: 0.715\n",
      "pt-PT: 0.62\n",
      "es-ES: 0.915\n",
      "id: 0.545\n",
      "\n",
      "fp: 610.0\n",
      "tp: 1777.0\n",
      "fn: 823.0\n",
      "tn: 30590.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bg': {'fn': 4, 'fp': 0, 'tn': 2400, 'tp': 196},\n",
       " 'bs': {'fn': 156, 'fp': 42, 'tn': 2358, 'tp': 44},\n",
       " 'cz': {'fn': 5, 'fp': 1, 'tn': 2399, 'tp': 195},\n",
       " 'es-AR': {'fn': 129, 'fp': 27, 'tn': 2373, 'tp': 71},\n",
       " 'es-ES': {'fn': 17, 'fp': 129, 'tn': 2271, 'tp': 183},\n",
       " 'hr': {'fn': 76, 'fp': 74, 'tn': 2326, 'tp': 124},\n",
       " 'id': {'fn': 91, 'fp': 62, 'tn': 2338, 'tp': 109},\n",
       " 'mk': {'fn': 0, 'fp': 0, 'tn': 2400, 'tp': 200},\n",
       " 'my': {'fn': 57, 'fp': 5, 'tn': 2395, 'tp': 143},\n",
       " 'pt-BR': {'fn': 105, 'fp': 63, 'tn': 2337, 'tp': 95},\n",
       " 'pt-PT': {'fn': 76, 'fp': 92, 'tn': 2308, 'tp': 124},\n",
       " 'sk': {'fn': 5, 'fp': 5, 'tn': 2395, 'tp': 195},\n",
       " 'sr': {'fn': 102, 'fp': 110, 'tn': 2290, 'tp': 98}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp=0.0\n",
    "total_fp=0.0\n",
    "total_fn=0.0\n",
    "total_tn=0.0\n",
    "total_precision=0.0\n",
    "total_recall=0.0\n",
    "total_f1score=0.0\n",
    "for key in metrics.keys():\n",
    "    tp=metrics[key][\"tp\"]\n",
    "    fp=metrics[key][\"fp\"]\n",
    "    fn=metrics[key][\"fn\"]\n",
    "    tn=metrics[key][\"tn\"]\n",
    "    precision=tp/float(tp+fp)\n",
    "    recall=tp/float(tp+fn)\n",
    "    f1_score=(2*recall*precision)/float(recall+precision)\n",
    "    total_precision+=precision\n",
    "    total_recall+=recall\n",
    "    total_f1score+=f1_score\n",
    "    \n",
    "    total_tp+=tp\n",
    "    total_fp+=fp\n",
    "    total_fn+=fn\n",
    "    total_tn+=tn\n",
    "    \n",
    "    #calc macro\n",
    "#calc micro\n",
    "mic_prec=total_tp/float(total_tp+total_fp)\n",
    "mic_recall=total_tp/float(total_tp+total_fn)\n",
    "print(\"Micro-averaged precision: \" + str(mic_prec))\n",
    "print(\"Micro-averaged recall: \" + str(mic_recall))\n",
    "print(\"Micro-averaged f1-score: \" + str((2*mic_prec*mic_recall)/float(mic_prec+mic_recall)))\n",
    "print(\"\")\n",
    "print(\"Macro-averaged precision: \" + str(total_precision/13.0))\n",
    "print(\"Macro-averaged recall: \" + str(total_recall/13.0))\n",
    "print(\"Macro-averaged f1-score: \" + str(total_f1score/13.0))\n",
    "print(\"\")\n",
    "print(\"Total accuracy: \"+ str(total_tp/2600.0))\n",
    "print(\"Accuracies for languages:\")\n",
    "for key,value in metrics.iteritems():\n",
    "    print(key + str(\": \")+str(value[\"tp\"]/200.0))\n",
    "print(\"\")\n",
    "\n",
    "print(\"fp: \"+str(total_fp))\n",
    "print(\"tp: \"+str(total_tp))\n",
    "print(\"fn: \"+str(total_fn))\n",
    "print(\"tn: \"+str(total_tn))\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\t196\t0\t2400\t4\n",
      "hr\t124\t74\t2326\t76\n",
      "es-AR\t71\t27\t2373\t129\n",
      "mk\t200\t0\t2400\t0\n",
      "sk\t195\t5\t2395\t5\n",
      "cz\t195\t1\t2399\t5\n",
      "sr\t98\t110\t2290\t102\n",
      "pt-BR\t95\t63\t2337\t105\n",
      "bs\t44\t42\t2358\t156\n",
      "my\t143\t5\t2395\t57\n",
      "pt-PT\t124\t92\t2308\t76\n",
      "es-ES\t183\t129\t2271\t17\n",
      "id\t109\t62\t2338\t91\n"
     ]
    }
   ],
   "source": [
    "for key,value in metrics.iteritems():\n",
    "    print(key + \"\\t\"+ str(value[\"tp\"]) + \"\\t\"+ str(value[\"fp\"])+\"\\t\"+str(value[\"tn\"])+\"\\t\"+str(value[\"fn\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for languages:\n",
      "0.98\n",
      "0.62\n",
      "0.355\n",
      "1.0\n",
      "0.975\n",
      "0.975\n",
      "0.49\n",
      "0.475\n",
      "0.22\n",
      "0.715\n",
      "0.62\n",
      "0.915\n",
      "0.545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies for languages:\")\n",
    "for key,value in metrics.iteritems():\n",
    "    print(str(value[\"tp\"]/200.0))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
