{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,subprocess\n",
    "from enum import Enum\n",
    "from collections import Counter\n",
    "\n",
    "    \n",
    "def generate_model_line(s,idx):\n",
    "    feature_list=[]\n",
    "    raw_sentence=s.split('\\t')[0]\n",
    "    sentence=[x for x in list(''.join(set(''.join(raw_sentence)))) if x not in insignificant_tokens]\n",
    "    model_line =str(idx+1)\n",
    "    #add unigrams\n",
    "    for ch in sentence:\n",
    "        feature_list.append(character_numbers[ch])\n",
    "        \n",
    "    #add bigrams\n",
    "    bigrams=set()\n",
    "    for words in raw_sentence.split(' '):\n",
    "        bigrams.update([x.lower() for x in[words[i:i+2] for i in range(len(words)-1)]])\n",
    "    bigrams_list=list([x for x in bigrams if (x[0] not in insignificant_tokens)and(x[1] not in insignificant_tokens)])\n",
    "    for bg in bigrams_list:\n",
    "        feature_list.append(character_numbers[bg])\n",
    "    \n",
    "    feature_list=list(set(feature_list))\n",
    "    feature_list.sort()\n",
    "    for ch in feature_list:\n",
    "        model_line+=\" \"+ str(ch)+ \":\" + str(1)\n",
    "    return model_line+\"\\n\"\n",
    "\n",
    "insignificant_tokens=[' ','!', '\"','#','$','%','&','*','+','-','(',')',',','.','/','0','1','2','3','4','5','6','7','8',\n",
    "                      '9',';','<','>','=','?','@','|','«','»','`','[',']',\"'\",'\\\\']\n",
    "language_ids=['bg','bs','cz','es-AR','es-ES','hr','id','mk','my','pt-BR','pt-PT','sk','sr']\n",
    "with open (\"Corpus/Raw Corpus.txt\") as f:\n",
    "    corpus = f.readlines()\n",
    "languages=[]\n",
    "corpus_s=list(''.join(set(''.join(corpus))))\n",
    "corpus_s=[x for x in corpus_s if x not in insignificant_tokens]\n",
    "character_numbers={}\n",
    "i=0\n",
    "for s in corpus_s:\n",
    "    i+=1\n",
    "    character_numbers[s]=i\n",
    "\n",
    "bigrams=set()\n",
    "for s in corpus:\n",
    "    for words in s.split(' '):\n",
    "        bigrams.update([x.lower() for x in[words[i:i+2] for i in range(len(words)-1)]])\n",
    "bigrams_list=list([x for x in bigrams if (x[0] not in insignificant_tokens)and(x[1] not in insignificant_tokens)])\n",
    "for bg in bigrams_list:\n",
    "    i+=1\n",
    "    character_numbers[bg]=i\n",
    "for i in range(13): #divides data set as languages\n",
    "    languages.append(\"\")\n",
    "    start=i*2000\n",
    "    end=(i+1)*2000\n",
    "    languages[i]=corpus[start:end] #2k sentence for each language\n",
    "training_set=[]\n",
    "test_set=[]\n",
    "training_model=[]\n",
    "test_model=[]\n",
    "for idx,l in enumerate(languages):\n",
    "    random.shuffle(l)\n",
    "    \n",
    "    training_partition=l[0:1800]\n",
    "    training_set.extend(training_partition)\n",
    "    \n",
    "    for s in training_partition:\n",
    "        training_model.append(generate_model_line(s,idx))\n",
    "    \n",
    "    test_partition=l[1800:2000]\n",
    "    test_set.extend(test_partition)\n",
    "    \n",
    "    for s in test_partition:\n",
    "        test_model.append(generate_model_line(s,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23400\n",
      "2600\n"
     ]
    }
   ],
   "source": [
    "def write_files(file_name,array):\n",
    "    with open (file_name, mode='wt') as t_file:\n",
    "        for item in array:\n",
    "            t_file.write(item)\n",
    "print(len(training_set))\n",
    "print(len(test_set))\n",
    "write_files(\"SVM/IncreasedFeatures/TrainingModel-SVM.txt\",training_model)\n",
    "write_files(\"SVM/IncreasedFeatures/TrainingSet-SVM.txt\",training_set)\n",
    "write_files(\"SVM/IncreasedFeatures/TestSet-SVM.txt\",test_set)\n",
    "write_files(\"SVM/IncreasedFeatures/TestModel-SVM.txt\",test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=subprocess.Popen(['SVM/svm_multiclass_learn', '-c', '5000' ,'SVM/IncreasedFeatures/TrainingModel-SVM.txt' ,'SVM/IncreasedFeatures/Model'],\n",
    "                 stdout=subprocess.PIPE)\n",
    "#for line in p.stdout:\n",
    "#    print(line)\n",
    "p.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model...done.\n",
      "\n",
      "Reading test examples... (2600 examples) done.\n",
      "\n",
      "Classifying test examples...done\n",
      "\n",
      "Runtime (without IO) in cpu-seconds: 0.03\n",
      "\n",
      "Average loss on test set: 20.3462\n",
      "\n",
      "Zero/one-error on test set: 20.35% (2071 correct, 529 incorrect, 2600 total)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=subprocess.Popen(['SVM/svm_multiclass_classify', 'SVM/IncreasedFeatures/TestModel-SVM.txt' ,'SVM/IncreasedFeatures/Model'],\n",
    "                 stdout=subprocess.PIPE)\n",
    "for line in p.stdout:\n",
    "    print(line)\n",
    "p.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
