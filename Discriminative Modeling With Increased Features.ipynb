{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,subprocess\n",
    "from enum import Enum\n",
    "from collections import Counter\n",
    "\n",
    "    \n",
    "def generate_model_line(s,idx): #generates svm input vectors using predefined character numbers\n",
    "    feature_list=[]\n",
    "    raw_sentence=s.split('\\t')[0]\n",
    "    sentence=[x for x in list(''.join(set(''.join(raw_sentence)))) if x not in insignificant_tokens]\n",
    "    model_line =str(idx+1)\n",
    "    #add unigrams\n",
    "    for ch in sentence:\n",
    "        feature_list.append(character_numbers[ch])\n",
    "        \n",
    "    #add bigrams\n",
    "    bigrams=set()\n",
    "    for words in raw_sentence.split(' '):\n",
    "        bigrams.update([x.lower() for x in[words[i:i+2] for i in range(len(words)-1)]])\n",
    "    bigrams_list=list([x for x in bigrams if (x[0] not in insignificant_tokens)and(x[1] not in insignificant_tokens)])\n",
    "    for bg in bigrams_list:\n",
    "        feature_list.append(character_numbers[bg])\n",
    "    \n",
    "    feature_list=list(set(feature_list))\n",
    "    feature_list.sort()\n",
    "    for ch in feature_list:\n",
    "        model_line+=\" \"+ str(ch)+ \":\" + str(1)\n",
    "    return model_line+\"\\n\"\n",
    "\n",
    "#these tokens will be skipped for both creating unigrams and bigrams  \n",
    "insignificant_tokens=[' ','!', '\"','#','$','%','&','*','+','-','(',')',',','.','/','0','1','2','3','4','5','6','7','8',\n",
    "                      '9',';','<','>','=','?','@','|','«','»','`','[',']',\"'\",'\\\\']\n",
    "language_ids=['bg','bs','cz','es-AR','es-ES','hr','id','mk','my','pt-BR','pt-PT','sk','sr']\n",
    "with open (\"Corpus/Raw Corpus.txt\") as f:\n",
    "    corpus = f.readlines()\n",
    "languages=[]\n",
    "corpus_s=list(''.join(set(''.join(corpus))))\n",
    "corpus_s=[x for x in corpus_s if x not in insignificant_tokens]\n",
    "character_numbers={}\n",
    "i=0\n",
    "for s in corpus_s:\n",
    "    i+=1\n",
    "    character_numbers[s]=i \n",
    "\n",
    "bigrams=set() #create bigrams as set to for uniqueness\n",
    "for s in corpus:\n",
    "    for words in s.split(' '):\n",
    "        bigrams.update([x.lower() for x in[words[i:i+2] for i in range(len(words)-1)]]) #add each bigrams in the sentences\n",
    "#skip bigrams contains skipped tokens \n",
    "bigrams_list=list([x for x in bigrams if (x[0] not in insignificant_tokens)and(x[1] not in insignificant_tokens)])\n",
    "for bg in bigrams_list:\n",
    "    i+=1\n",
    "    character_numbers[bg]=i\n",
    "for i in range(13): #divides data set as languages\n",
    "    languages.append(\"\")\n",
    "    start=i*2000\n",
    "    end=(i+1)*2000\n",
    "    languages[i]=corpus[start:end] #2k sentence for each language\n",
    "training_set=[]\n",
    "test_set=[]\n",
    "training_model=[]\n",
    "test_model=[]\n",
    "for idx,l in enumerate(languages):\n",
    "    random.shuffle(l)\n",
    "    \n",
    "    training_partition=l[0:1800]\n",
    "    training_set.extend(training_partition)\n",
    "    \n",
    "    for s in training_partition:\n",
    "        training_model.append(generate_model_line(s,idx))\n",
    "    \n",
    "    test_partition=l[1800:2000]\n",
    "    test_set.extend(test_partition)\n",
    "    \n",
    "    for s in test_partition:\n",
    "        test_model.append(generate_model_line(s,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23400\n",
      "2600\n"
     ]
    }
   ],
   "source": [
    "def write_files(file_name,array):\n",
    "    with open (file_name, mode='wt') as t_file:\n",
    "        for item in array:\n",
    "            t_file.write(item)\n",
    "print(len(training_set))\n",
    "print(len(test_set))\n",
    "write_files(\"SVM/IncreasedFeatures/TrainingModel-SVM.txt\",training_model)\n",
    "write_files(\"SVM/IncreasedFeatures/TrainingSet-SVM.txt\",training_set)\n",
    "write_files(\"SVM/IncreasedFeatures/TestSet-SVM.txt\",test_set)\n",
    "write_files(\"SVM/IncreasedFeatures/TestModel-SVM.txt\",test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=subprocess.Popen(['SVM/svm_multiclass_learn', '-c', '5000' ,'SVM/IncreasedFeatures/TrainingModel-SVM.txt' ,'SVM/IncreasedFeatures/Model'],\n",
    "                 stdout=subprocess.PIPE)\n",
    "\n",
    "p.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model...done.\n",
      "\n",
      "Reading test examples... (2600 examples) done.\n",
      "\n",
      "Classifying test examples...done\n",
      "\n",
      "Runtime (without IO) in cpu-seconds: 0.02\n",
      "\n",
      "Average loss on test set: 21.1923\n",
      "\n",
      "Zero/one-error on test set: 21.19% (2049 correct, 551 incorrect, 2600 total)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=subprocess.Popen(['SVM/svm_multiclass_classify', 'SVM/IncreasedFeatures/TestModel-SVM.txt' ,'SVM/IncreasedFeatures/Model','SVM/IncreasedFeatures/predictions.txt'],\n",
    "                 stdout=subprocess.PIPE)\n",
    "for line in p.stdout:\n",
    "    print(line)\n",
    "p.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\n",
      "True positives: 199\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 1\n",
      "hr\n",
      "True positives: 146\n",
      "False positives: 21\n",
      "True negatives: 2379\n",
      "False negatives: 54\n",
      "es-AR\n",
      "True positives: 130\n",
      "False positives: 50\n",
      "True negatives: 2350\n",
      "False negatives: 70\n",
      "mk\n",
      "True positives: 200\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 0\n",
      "sk\n",
      "True positives: 200\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 0\n",
      "cz\n",
      "True positives: 198\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 2\n",
      "sr\n",
      "True positives: 157\n",
      "False positives: 96\n",
      "True negatives: 2304\n",
      "False negatives: 43\n",
      "pt-BR\n",
      "True positives: 147\n",
      "False positives: 66\n",
      "True negatives: 2334\n",
      "False negatives: 53\n",
      "bs\n",
      "True positives: 74\n",
      "False positives: 48\n",
      "True negatives: 2352\n",
      "False negatives: 126\n",
      "my\n",
      "True positives: 164\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 36\n",
      "pt-PT\n",
      "True positives: 133\n",
      "False positives: 52\n",
      "True negatives: 2348\n",
      "False negatives: 67\n",
      "es-ES\n",
      "True positives: 149\n",
      "False positives: 67\n",
      "True negatives: 2333\n",
      "False negatives: 51\n",
      "id\n",
      "True positives: 152\n",
      "False positives: 37\n",
      "True negatives: 2363\n",
      "False negatives: 48\n"
     ]
    }
   ],
   "source": [
    "with open (\"SVM/IncreasedFeatures/predictions.txt\") as f:\n",
    "    predictions = f.readlines()\n",
    "predictions=[i.split(' ')[0] for i in predictions]\n",
    "metrics={}\n",
    "for idx,lang_id in enumerate(language_ids):\n",
    "    false_negatives=len([x for x in predictions[idx*200:(idx+1)*200] if int(x) != (idx+1)])\n",
    "    true_positives=200-false_negatives\n",
    "    if idx==0: #count false positives on proceeding predictions\n",
    "        false_positives=len([x for x in predictions[200:len(predictions)] if int(x) ==1])\n",
    "    elif idx==12:  #count false positives on preceeding predictions\n",
    "        false_positives=len([x for x in predictions[0:2400] if int(x)==13])\n",
    "    else: #count false positives on both preceeding and proceeding predictions\n",
    "        false_positives=[x for x in predictions[(idx-1)*200:idx*200] if int(x) == (idx+1)]\n",
    "        false_positives=len(false_positives + [x for x in predictions[(idx+1)*200:2600] if int(x) == (idx+1)])\n",
    "    true_negatives=2400-false_positives\n",
    "    metrics[lang_id]={\"tp\":true_positives,\"fn\":false_negatives,\"tn\":true_negatives,\"fp\":false_positives}\n",
    "for key,value in metrics.iteritems():\n",
    "    print(key)\n",
    "    print(\"True positives: \" + str(value[\"tp\"]))\n",
    "    print(\"False positives: \" + str(value[\"fp\"]))\n",
    "    print(\"True negatives: \" + str(value[\"tn\"]))\n",
    "    print(\"False negatives: \" + str(value[\"fn\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged precision: 0.824215607401\n",
      "Micro-averaged recall: 0.788076923077\n",
      "Micro-averaged f1-score: 0.805741250492\n",
      "\n",
      "Macro-averaged precision: 0.825130141461\n",
      "Macro-averaged recall: 0.788076923077\n",
      "Macro-averaged f1-score: 0.802062544561\n",
      "\n",
      "Total accuracy: 0.788076923077\n",
      "Accuracies for languages:\n",
      "bg: 0.995\n",
      "hr: 0.73\n",
      "es-AR: 0.65\n",
      "mk: 1.0\n",
      "sk: 1.0\n",
      "cz: 0.99\n",
      "sr: 0.785\n",
      "pt-BR: 0.735\n",
      "bs: 0.37\n",
      "my: 0.82\n",
      "pt-PT: 0.665\n",
      "es-ES: 0.745\n",
      "id: 0.76\n",
      "\n",
      "fp: 437.0\n",
      "tp: 2049.0\n",
      "fn: 551.0\n",
      "tn: 30763.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bg': {'fn': 1, 'fp': 0, 'tn': 2400, 'tp': 199},\n",
       " 'bs': {'fn': 126, 'fp': 48, 'tn': 2352, 'tp': 74},\n",
       " 'cz': {'fn': 2, 'fp': 0, 'tn': 2400, 'tp': 198},\n",
       " 'es-AR': {'fn': 70, 'fp': 50, 'tn': 2350, 'tp': 130},\n",
       " 'es-ES': {'fn': 51, 'fp': 67, 'tn': 2333, 'tp': 149},\n",
       " 'hr': {'fn': 54, 'fp': 21, 'tn': 2379, 'tp': 146},\n",
       " 'id': {'fn': 48, 'fp': 37, 'tn': 2363, 'tp': 152},\n",
       " 'mk': {'fn': 0, 'fp': 0, 'tn': 2400, 'tp': 200},\n",
       " 'my': {'fn': 36, 'fp': 0, 'tn': 2400, 'tp': 164},\n",
       " 'pt-BR': {'fn': 53, 'fp': 66, 'tn': 2334, 'tp': 147},\n",
       " 'pt-PT': {'fn': 67, 'fp': 52, 'tn': 2348, 'tp': 133},\n",
       " 'sk': {'fn': 0, 'fp': 0, 'tn': 2400, 'tp': 200},\n",
       " 'sr': {'fn': 43, 'fp': 96, 'tn': 2304, 'tp': 157}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp=0.0\n",
    "total_fp=0.0\n",
    "total_fn=0.0\n",
    "total_tn=0.0\n",
    "total_precision=0.0\n",
    "total_recall=0.0\n",
    "total_f1score=0.0\n",
    "for key in metrics.keys():\n",
    "    tp=metrics[key][\"tp\"]\n",
    "    fp=metrics[key][\"fp\"]\n",
    "    fn=metrics[key][\"fn\"]\n",
    "    tn=metrics[key][\"tn\"]\n",
    "    precision=tp/float(tp+fp)\n",
    "    recall=tp/float(tp+fn)\n",
    "    f1_score=(2*recall*precision)/float(recall+precision)\n",
    "    total_precision+=precision\n",
    "    total_recall+=recall\n",
    "    total_f1score+=f1_score\n",
    "    \n",
    "    total_tp+=tp\n",
    "    total_fp+=fp\n",
    "    total_fn+=fn\n",
    "    total_tn+=tn\n",
    "    \n",
    "mic_prec=total_tp/float(total_tp+total_fp)\n",
    "mic_recall=total_tp/float(total_tp+total_fn)\n",
    "print(\"Micro-averaged precision: \" + str(mic_prec))\n",
    "print(\"Micro-averaged recall: \" + str(mic_recall))\n",
    "print(\"Micro-averaged f1-score: \" + str((2*mic_prec*mic_recall)/float(mic_prec+mic_recall)))\n",
    "print(\"\")\n",
    "print(\"Macro-averaged precision: \" + str(total_precision/13.0))\n",
    "print(\"Macro-averaged recall: \" + str(total_recall/13.0))\n",
    "print(\"Macro-averaged f1-score: \" + str(total_f1score/13.0))\n",
    "print(\"\")\n",
    "print(\"Total accuracy: \"+ str(total_tp/2600.0))\n",
    "print(\"Accuracies for languages:\")\n",
    "for key,value in metrics.iteritems():\n",
    "    print(key + str(\": \")+str(value[\"tp\"]/200.0))\n",
    "print(\"\")\n",
    "print(\"fp: \"+str(total_fp))\n",
    "print(\"tp: \"+str(total_tp))\n",
    "print(\"fn: \"+str(total_fn))\n",
    "print(\"tn: \"+str(total_tn))\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\t199\t0\t2400\t1\n",
      "hr\t146\t21\t2379\t54\n",
      "es-AR\t130\t50\t2350\t70\n",
      "mk\t200\t0\t2400\t0\n",
      "sk\t200\t0\t2400\t0\n",
      "cz\t198\t0\t2400\t2\n",
      "sr\t157\t96\t2304\t43\n",
      "pt-BR\t147\t66\t2334\t53\n",
      "bs\t74\t48\t2352\t126\n",
      "my\t164\t0\t2400\t36\n",
      "pt-PT\t133\t52\t2348\t67\n",
      "es-ES\t149\t67\t2333\t51\n",
      "id\t152\t37\t2363\t48\n"
     ]
    }
   ],
   "source": [
    "for key,value in metrics.iteritems():\n",
    "    print(key + \"\\t\"+ str(value[\"tp\"]) + \"\\t\"+ str(value[\"fp\"])+\"\\t\"+str(value[\"tn\"])+\"\\t\"+str(value[\"fn\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for languages:\n",
      "0.995\n",
      "0.73\n",
      "0.65\n",
      "1.0\n",
      "1.0\n",
      "0.99\n",
      "0.785\n",
      "0.735\n",
      "0.37\n",
      "0.82\n",
      "0.665\n",
      "0.745\n",
      "0.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies for languages:\")\n",
    "for key,value in metrics.iteritems():\n",
    "    print(str(value[\"tp\"]/200.0))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
