{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#these tokens will be skipped while calculating probability\n",
    "insignificant_tokens=[' ','!', '\"','#','$','%','&','*','+','-','(',')',',','.','/','0','1','2','3','4','5','6','7','8',\n",
    "                      '9',';','<','>','=','?','@','|','«','»','`','[',']',\"'\",'\\\\']\n",
    "language_ids=['bg','bs','cz','es-AR','es-ES','hr','id','mk','my','pt-BR','pt-PT','sk','sr']\n",
    "\n",
    "class Language: #for each language in data set, we create a Language class\n",
    "   \n",
    "    def __init__ (self,lang_id):\n",
    "        self.chars={} # storing characters and their counts\n",
    "        self.prob={} #storing p(c|l) probability for each character\n",
    "        # corresponding language id \n",
    "        self.lang_id=lang_id\n",
    "    def calculate_probability(self,sentence):\n",
    "        s=list(sentence)\n",
    "        s=[x for x in s if x not in insignificant_tokens]\n",
    "        total=0\n",
    "        for ch in s:\n",
    "            if self.chars.has_key(ch):\n",
    "                total+=self.prob[ch]\n",
    "            else: \n",
    "                return 0\n",
    "        return total\n",
    "    def get_chars(self):\n",
    "        return self.chars\n",
    "    def get_prob(self):\n",
    "        return self.prob\n",
    "    def get_lang_id(self):\n",
    "        return self.lang_id\n",
    "with open (\"Corpus/Raw Corpus.txt\") as f:\n",
    "    corpus = f.readlines()\n",
    "\n",
    "\n",
    "languages=[]\n",
    "model=[]\n",
    "for i in range(13): #divides data set as languages\n",
    "    languages.append(\"\")\n",
    "    start=i*2000\n",
    "    end=(i+1)*2000\n",
    "    languages[i]=corpus[start:end] #2k sentence for each language\n",
    "\n",
    "training_set=[]\n",
    "test_set=[]\n",
    "for idx,l in enumerate(languages):\n",
    "    lang=Language(language_ids[idx])\n",
    "    random.shuffle(l)     \n",
    "    \n",
    "    training_partition=l[0:1800]\n",
    "    training_set.extend(training_partition)\n",
    " \n",
    "    training_partition=[i.split('\\t')[0] for i in training_partition] #remove language identifier at the last of the sentences\n",
    "    for sentence in training_partition:\n",
    "        for letter in sentence:\n",
    "            if letter not in insignificant_tokens:\n",
    "                if lang.get_chars().has_key(letter):\n",
    "                    nominator=lang.get_chars()[letter]\n",
    "                else:\n",
    "                    nominator=0 #laplace \n",
    "                nominator+=1\n",
    "                lang.get_chars()[letter]=nominator\n",
    "                \n",
    "    test_partition=l[1800:2000]\n",
    "    test_set.extend(test_partition)\n",
    "    test_part=[i.split('\\t')[0] for i in test_partition] #remove language identifier at the last of the sentences\n",
    "    \n",
    "    unk=list(''.join(set(''.join(test_part))))\n",
    "    unk=[x for x in unk if x not in insignificant_tokens]\n",
    "    unk=[x for x in unk if x not in lang.get_chars().keys()]\n",
    "    if len(unk)!=0: # add unknowns characters in test set for smoothing\n",
    "        for l in unk:\n",
    "            lang.get_chars()[l]=0\n",
    "        \n",
    "    for ch in lang.chars: #laplace smoothing\n",
    "        lang.get_chars()[ch]+=1\n",
    "    \n",
    "    denominator=sum(lang.get_chars().values())+len(lang.get_chars().values()) #for performance\n",
    "    for letter in lang.get_chars(): #calculate probabilities\n",
    "        lang.get_prob()[letter]=(lang.get_chars()[letter]+1)/float(denominator)\n",
    "  \n",
    "\n",
    "    model.append(lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "predictions=[]\n",
    "expected=[]\n",
    "for sentence in test_set:\n",
    "    s=sentence.split('\\t')\n",
    "    probabilities={}\n",
    "    expected.append(language_ids.index(s[1].strip())+1)\n",
    "    for l_model in model: #foreach language calculate the probability of the given sentence\n",
    "        probabilities[l_model.get_lang_id()]=l_model.calculate_probability(s[0])\n",
    "    #find one language having most likely given sentence     \n",
    "    predictions.append(language_ids.index(max(probabilities.items(), key=lambda k: k[1])[0])+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\n",
      "True positives: 45\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 155\n",
      "hr\n",
      "True positives: 28\n",
      "False positives: 7\n",
      "True negatives: 2393\n",
      "False negatives: 172\n",
      "es-AR\n",
      "True positives: 1\n",
      "False positives: 42\n",
      "True negatives: 2358\n",
      "False negatives: 199\n",
      "mk\n",
      "True positives: 200\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 0\n",
      "sk\n",
      "True positives: 160\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 40\n",
      "cz\n",
      "True positives: 21\n",
      "False positives: 0\n",
      "True negatives: 2400\n",
      "False negatives: 179\n",
      "sr\n",
      "True positives: 8\n",
      "False positives: 2\n",
      "True negatives: 2398\n",
      "False negatives: 192\n",
      "pt-BR\n",
      "True positives: 171\n",
      "False positives: 166\n",
      "True negatives: 2234\n",
      "False negatives: 29\n",
      "bs\n",
      "True positives: 170\n",
      "False positives: 357\n",
      "True negatives: 2043\n",
      "False negatives: 30\n",
      "my\n",
      "True positives: 200\n",
      "False positives: 15\n",
      "True negatives: 2385\n",
      "False negatives: 0\n",
      "pt-PT\n",
      "True positives: 9\n",
      "False positives: 5\n",
      "True negatives: 2395\n",
      "False negatives: 191\n",
      "es-ES\n",
      "True positives: 197\n",
      "False positives: 228\n",
      "True negatives: 2172\n",
      "False negatives: 3\n",
      "id\n",
      "True positives: 1\n",
      "False positives: 2\n",
      "True negatives: 2398\n",
      "False negatives: 199\n"
     ]
    }
   ],
   "source": [
    "metrics={}\n",
    "for idx,lang_id in enumerate(language_ids):\n",
    "    false_negatives=len([x for x in predictions[idx*200:(idx+1)*200] if int(x) != (idx+1)])\n",
    "    true_positives=200-false_negatives\n",
    "    if idx==0: #count false positives on proceeding predictions\n",
    "        false_positives=len([x for x in predictions[200:len(predictions)] if int(x) ==1])\n",
    "    elif idx==12:  #count false positives on preceeding predictions\n",
    "        false_positives=len([x for x in predictions[0:2400] if int(x)==13])\n",
    "    else: #count false positives on both preceeding and proceeding predictions\n",
    "        false_positives=[x for x in predictions[(idx-1)*200:idx*200] if int(x) == (idx+1)]\n",
    "        false_positives=len(false_positives + [x for x in predictions[(idx+1)*200:2600] if int(x) == (idx+1)])\n",
    "    true_negatives=2400-false_positives\n",
    "    metrics[lang_id]={\"tp\":true_positives,\"fn\":false_negatives,\"tn\":true_negatives,\"fp\":false_positives}\n",
    "for key,value in metrics.iteritems():\n",
    "    print(key)\n",
    "    print(\"True positives: \" + str(value[\"tp\"]))\n",
    "    print(\"False positives: \" + str(value[\"fp\"]))\n",
    "    print(\"True negatives: \" + str(value[\"tn\"]))\n",
    "    print(\"False negatives: \" + str(value[\"fn\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged precision: 0.595085995086\n",
      "Micro-averaged recall: 0.465769230769\n",
      "Micro-averaged f1-score: 0.522545846818\n",
      "\n",
      "Macro-averaged precision: 0.678708254064\n",
      "Macro-averaged recall: 0.465769230769\n",
      "Macro-averaged f1-score: 0.427828196335\n",
      "\n",
      "Total accuracy: 0.465769230769\n",
      "Accuracies for languages:\n",
      "bg: 0.225\n",
      "hr: 0.14\n",
      "es-AR: 0.005\n",
      "mk: 1.0\n",
      "sk: 0.8\n",
      "cz: 0.105\n",
      "sr: 0.04\n",
      "pt-BR: 0.855\n",
      "bs: 0.85\n",
      "my: 1.0\n",
      "pt-PT: 0.045\n",
      "es-ES: 0.985\n",
      "id: 0.005\n",
      "\n",
      "fp: 824.0\n",
      "tp: 1211.0\n",
      "fn: 1389.0\n",
      "tn: 30376.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bg': {'fn': 155, 'fp': 0, 'tn': 2400, 'tp': 45},\n",
       " 'bs': {'fn': 30, 'fp': 357, 'tn': 2043, 'tp': 170},\n",
       " 'cz': {'fn': 179, 'fp': 0, 'tn': 2400, 'tp': 21},\n",
       " 'es-AR': {'fn': 199, 'fp': 42, 'tn': 2358, 'tp': 1},\n",
       " 'es-ES': {'fn': 3, 'fp': 228, 'tn': 2172, 'tp': 197},\n",
       " 'hr': {'fn': 172, 'fp': 7, 'tn': 2393, 'tp': 28},\n",
       " 'id': {'fn': 199, 'fp': 2, 'tn': 2398, 'tp': 1},\n",
       " 'mk': {'fn': 0, 'fp': 0, 'tn': 2400, 'tp': 200},\n",
       " 'my': {'fn': 0, 'fp': 15, 'tn': 2385, 'tp': 200},\n",
       " 'pt-BR': {'fn': 29, 'fp': 166, 'tn': 2234, 'tp': 171},\n",
       " 'pt-PT': {'fn': 191, 'fp': 5, 'tn': 2395, 'tp': 9},\n",
       " 'sk': {'fn': 40, 'fp': 0, 'tn': 2400, 'tp': 160},\n",
       " 'sr': {'fn': 192, 'fp': 2, 'tn': 2398, 'tp': 8}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tp=0.0\n",
    "total_fp=0.0\n",
    "total_fn=0.0\n",
    "total_tn=0.0\n",
    "total_precision=0.0\n",
    "total_recall=0.0\n",
    "total_f1score=0.0\n",
    "for key in metrics.keys():\n",
    "    tp=metrics[key][\"tp\"]\n",
    "    fp=metrics[key][\"fp\"]\n",
    "    fn=metrics[key][\"fn\"]\n",
    "    tn=metrics[key][\"tn\"]\n",
    "    precision=tp/float(tp+fp)\n",
    "    recall=tp/float(tp+fn)\n",
    "    f1_score=(2*recall*precision)/float(recall+precision)\n",
    "    total_precision+=precision\n",
    "    total_recall+=recall\n",
    "    total_f1score+=f1_score\n",
    "    \n",
    "    total_tp+=tp\n",
    "    total_fp+=fp\n",
    "    total_fn+=fn\n",
    "    total_tn+=tn\n",
    "    \n",
    "mic_prec=total_tp/float(total_tp+total_fp)\n",
    "mic_recall=total_tp/float(total_tp+total_fn)\n",
    "print(\"Micro-averaged precision: \" + str(mic_prec))\n",
    "print(\"Micro-averaged recall: \" + str(mic_recall))\n",
    "print(\"Micro-averaged f1-score: \" + str((2*mic_prec*mic_recall)/float(mic_prec+mic_recall)))\n",
    "print(\"\")\n",
    "print(\"Macro-averaged precision: \" + str(total_precision/13.0))\n",
    "print(\"Macro-averaged recall: \" + str(total_recall/13.0))\n",
    "print(\"Macro-averaged f1-score: \" + str(total_f1score/13.0))\n",
    "print(\"\")\n",
    "print(\"Total accuracy: \"+ str(total_tp/2600.0))\n",
    "print(\"Accuracies for languages:\")\n",
    "for key,value in metrics.iteritems():\n",
    "    print(key + str(\": \")+str(value[\"tp\"]/200.0))\n",
    "print(\"\")\n",
    "print(\"fp: \"+str(total_fp))\n",
    "print(\"tp: \"+str(total_tp))\n",
    "print(\"fn: \"+str(total_fn))\n",
    "print(\"tn: \"+str(total_tn))\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_files(file_name,array):\n",
    "    with open (file_name, mode='wt') as t_file:\n",
    "        for item in array:\n",
    "            t_file.write(item)\n",
    "print(len(training_set))\n",
    "print(len(test_set))\n",
    "write_files(\"Training set.txt\",training_set)\n",
    "write_files(\"Test set.txt\",test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
